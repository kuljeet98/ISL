{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCN+RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh3DIR8X6lPk",
        "colab_type": "text"
      },
      "source": [
        "#### Indian sign laguages (CNN + RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn3xyUAZ22wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "b74c1747-92e9-4f6c-c59f-d3c5cec0a1ed"
      },
      "source": [
        "''' keras video genrator for data augmentation of the video'''\n",
        "!pip install keras-video-generators\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-video-generators in /usr/local/lib/python3.6/dist-packages (1.0.13)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-video-generators) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-video-generators) (1.18.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from keras-video-generators) (3.2.1)\n",
            "Requirement already satisfied: keras>=2 in /usr/local/lib/python3.6/dist-packages (from keras-video-generators) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-video-generators) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-video-generators) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-video-generators) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-video-generators) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2->keras-video-generators) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2->keras-video-generators) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2->keras-video-generators) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2->keras-video-generators) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2->keras-video-generators) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2->keras-video-generators) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->keras-video-generators) (46.0.0)\n",
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCI3L7zkr6Hu",
        "colab_type": "code",
        "outputId": "33efdea0-f133-463f-dddc-95e381f43cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/PROJECT_DATA"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PROJECT_DATA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB3Zas2w4Jab",
        "colab_type": "code",
        "outputId": "a2ed2123-1a9d-42a9-d78f-077b62566c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''importing libraries'''\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from keras_video import VideoFrameGenerator"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNtSMOUZkHo",
        "colab_type": "code",
        "outputId": "0fca1f1e-3a7f-480e-8e28-81fd548a4515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from keras_video import VideoFrameGenerator\n",
        "# use sub directories names as classes\n",
        "classes = [i.split(os.path.sep)[1] for i in glob.glob('Hands_only_OpenPose/*')]\n",
        "classes.sort()\n",
        "# some global params\n",
        "SIZE = (150, 150)\n",
        "CHANNELS = 3\n",
        "NBFRAME = 11\n",
        "BS = 8\n",
        "# pattern to get videos and classes\n",
        "glob_pattern='Hands_only_OpenPose/{classname}/*.mp4'\n",
        "# for data augmentation\n",
        "data_aug = keras.preprocessing.image.ImageDataGenerator(\n",
        "    zoom_range=.7)\n",
        "    # horizontal_flip=True,\n",
        "    # rotation_range=8,\n",
        "    # width_shift_range=.2,\n",
        "    # height_shift/_range=.2)\n",
        "# Create video frame generator\n",
        "train = VideoFrameGenerator(\n",
        "    classes=classes, \n",
        "    glob_pattern=glob_pattern,\n",
        "    nb_frames=NBFRAME,\n",
        "    split_val=.3, \n",
        "    shuffle=True,\n",
        "    batch_size=BS,\n",
        "    target_shape=SIZE,\n",
        "    nb_channel=CHANNELS,\n",
        "    transformation=data_aug,\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class friday, validation count: 30, train count: 72\n",
            "class monday, validation count: 34, train count: 81\n",
            "class saturday, validation count: 30, train count: 73\n",
            "class sunday, validation count: 38, train count: 90\n",
            "class thursday, validation count: 30, train count: 72\n",
            "class tuesday, validation count: 39, train count: 93\n",
            "class wednesday, validation count: 31, train count: 75\n",
            "Total data: 7 classes for 556 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaX9FiPasiWu",
        "colab_type": "code",
        "outputId": "077f2405-b364-4891-db84-d328a2215b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "valid = train.get_validation_generator()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 7 classes for 232 files for validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7_0wsMsupg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import keras_video.utils\n",
        "# keras_video.utils.show_sample(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4xVo0sC9Ie8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b0fZp9V1gAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a8e03988-3190-4aee-9982-392957e7b44c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(11, 150,150,3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_lst_m2d_4 (ConvLSTM2D)  (None, 150, 148, 64)      51712     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 150, 148, 64)      0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1420800)           0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               142080100 \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 7)                 707       \n",
            "=================================================================\n",
            "Total params: 142,132,519\n",
            "Trainable params: 142,132,519\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVEtjZ09_abA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# def get_weight(y):\n",
        "#     class_weight_current =  class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
        "#     return class_weight_current"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "84CwcNFVHFt_",
        "colab": {}
      },
      "source": [
        "# class_weight = get_weight(train.classes)\n",
        "# class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2hy95o6_h4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpFlYQ9O2-uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
        "filepath=\"/content/drive/My Drive/Models/openpose.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss',save_best_only=True, verbose=1)\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.1, patience=4, verbose=1)\n",
        "early_stopping = EarlyStopping( monitor='val_loss',patience=10,verbose=1,restore_best_weights=True)\n",
        "callbacks_list = [checkpoint, reduce_on_plateau, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WXiymue3R_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11c43099-84c1-40a7-dbfe-3448d0b9cecf"
      },
      "source": [
        "history = model.fit_generator(train, epochs=20,\n",
        "                              callbacks=callbacks_list, verbose = 1,\n",
        "                              validation_data = valid,\n",
        "                              workers=5)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "69/69 [==============================] - 109s 2s/step - loss: 1.8832 - acc: 0.2536 - val_loss: 1.9223 - val_acc: 0.2802\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.92226, saving model to /content/drive/My Drive/Models/openpose.hdf5\n",
            "Epoch 2/20\n",
            "69/69 [==============================] - 110s 2s/step - loss: 1.7305 - acc: 0.3279 - val_loss: 1.6510 - val_acc: 0.3922\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.92226 to 1.65099, saving model to /content/drive/My Drive/Models/openpose.hdf5\n",
            "Epoch 3/20\n",
            "69/69 [==============================] - 96s 1s/step - loss: 1.6602 - acc: 0.3641 - val_loss: 1.6711 - val_acc: 0.3621\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.65099\n",
            "Epoch 4/20\n",
            "69/69 [==============================] - 97s 1s/step - loss: 1.5883 - acc: 0.4022 - val_loss: 1.5474 - val_acc: 0.4741\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.65099 to 1.54739, saving model to /content/drive/My Drive/Models/openpose.hdf5\n",
            "Epoch 5/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.4830 - acc: 0.4366 - val_loss: 1.4690 - val_acc: 0.4828\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.54739 to 1.46896, saving model to /content/drive/My Drive/Models/openpose.hdf5\n",
            "Epoch 6/20\n",
            "69/69 [==============================] - 105s 2s/step - loss: 1.4208 - acc: 0.5036 - val_loss: 1.4900 - val_acc: 0.4655\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.46896\n",
            "Epoch 7/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.3281 - acc: 0.5344 - val_loss: 1.4728 - val_acc: 0.5259\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.46896\n",
            "Epoch 8/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.2488 - acc: 0.5652 - val_loss: 1.8614 - val_acc: 0.4784\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.46896\n",
            "Epoch 9/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.1329 - acc: 0.6141 - val_loss: 1.3446 - val_acc: 0.6034\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.46896 to 1.34462, saving model to /content/drive/My Drive/Models/openpose.hdf5\n",
            "Epoch 10/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.1752 - acc: 0.5996 - val_loss: 1.4528 - val_acc: 0.5474\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.34462\n",
            "Epoch 11/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.0409 - acc: 0.6395 - val_loss: 1.3185 - val_acc: 0.5776\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.34462 to 1.31845, saving model to /content/drive/My Drive/Models/openpose.hdf5\n",
            "Epoch 12/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 1.1062 - acc: 0.6178 - val_loss: 1.3346 - val_acc: 0.6078\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.31845\n",
            "Epoch 13/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.9827 - acc: 0.6685 - val_loss: 1.3339 - val_acc: 0.5948\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.31845\n",
            "Epoch 14/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.9082 - acc: 0.6957 - val_loss: 1.4157 - val_acc: 0.5991\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.31845\n",
            "Epoch 15/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.8798 - acc: 0.7029 - val_loss: 1.3311 - val_acc: 0.6121\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.31845\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 16/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.7973 - acc: 0.7409 - val_loss: 1.3457 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.31845\n",
            "Epoch 17/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.7990 - acc: 0.7301 - val_loss: 1.3952 - val_acc: 0.6379\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.31845\n",
            "Epoch 18/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.8289 - acc: 0.7174 - val_loss: 1.4002 - val_acc: 0.6207\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.31845\n",
            "Epoch 19/20\n",
            "69/69 [==============================] - 104s 2s/step - loss: 0.7520 - acc: 0.7518 - val_loss: 1.4201 - val_acc: 0.6336\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.31845\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 20/20\n",
            "69/69 [==============================] - 105s 2s/step - loss: 0.7421 - acc: 0.7446 - val_loss: 1.4203 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.31845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MBWusb23ebB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}